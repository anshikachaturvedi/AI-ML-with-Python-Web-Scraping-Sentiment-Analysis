{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DCoxlYHdiDT"
   },
   "source": [
    "**AI/ML with Python: Web Scraping & Sentiment Analysis**\n",
    "\n",
    "**Sentiment Analysis Tool**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kszldz2odw_w"
   },
   "source": [
    "**Introduction to VADER:**\n",
    "\n",
    "VADER is a sentiment analysis tool that is specifically attuned to sentiments expressed in social media and is available in the Natural Language Toolkit (NLTK) library for Python.The tool evaluates text to determine the sentiment of each lexical feature it encounters, adjusts the sentiment scores based on rules that consider syntax and grammatical conventions, and provides an overall sentiment score.\n",
    "\n",
    "The sentiment score given by VADER can be broken down into three components - positive, negative, and neutral, and also provides a compound score which is a normalized, weighted composite score. This compound score is often used as the singular measure of sentiment for a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI5l1yVUiIF2"
   },
   "source": [
    "We start off by importing **nltk** (Natural Language Toolkit) which allows us utilise its internal package **SentimentIntensityAnalyzer** that will provide us with the necessary polarity scores in terms of negative, neutral, or positive. To start, ensure that you have **ntlk** installed on your local machine. If you haven't, open your terminal and do **pip install nltk** as shown below.\n",
    "\n",
    "After importing **nltk**, ensure that you have **vader_lexicon** downloaded. Once everything is completed, we will proceed to import **SentimentIntensityAnalyzer** as a package from **nltk.sentiment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut64-0_O7ms7",
    "outputId": "14e57976-19e2-4b7f-8313-28faa7eaeace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "import nltk\n",
    "\n",
    "# Remember to download vader_lexicon if you havent! Simply uncomment the code below and run it\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PkjqNdXifoN"
   },
   "source": [
    "With our setup complete, we're now equipped to analyze the sentiment of various sentences. We will utilize the **polarity_scores** method to evaluate and display their sentiment metrics. Proceed with executing the following code to observe the breakdown of sentiment scores for each sentence provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1BNRVaK7pdJ",
    "outputId": "4ba741a9-ab10-4b84-880e-253acd28a784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product, it's absolutely amazing!\n",
      "Scores: {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.862}\n",
      "\n",
      "Text: This is the worst movie I have ever seen.\n",
      "Scores: {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n",
      "\n",
      "Text: I'm not sure how I feel about this new policy.\n",
      "Scores: {'neg': 0.197, 'neu': 0.803, 'pos': 0.0, 'compound': -0.2411}\n",
      "\n",
      "Text: Meh, it was okay, nothing special.\n",
      "Scores: {'neg': 0.421, 'neu': 0.355, 'pos': 0.225, 'compound': -0.1675}\n",
      "\n",
      "Text: Wow, this new update is fantastic! 😊\n",
      "Scores: {'neg': 0.0, 'neu': 0.342, 'pos': 0.658, 'compound': 0.8268}\n",
      "\n",
      "Text: I had an excellent day, but the weather was horrible.\n",
      "Scores: {'neg': 0.337, 'neu': 0.496, 'pos': 0.167, 'compound': -0.5267}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample texts that we will be using for sentiment analysis\n",
    "texts = [\n",
    "    \"I love this product, it's absolutely amazing!\",\n",
    "    \"This is the worst movie I have ever seen.\",\n",
    "    \"I'm not sure how I feel about this new policy.\",\n",
    "    \"Meh, it was okay, nothing special.\",\n",
    "    \"Wow, this new update is fantastic! 😊\",\n",
    "    \"I had an excellent day, but the weather was horrible.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    scores = sia.polarity_scores(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJVMBEe8708X",
    "outputId": "947d0c32-2ca8-4747-b273-ec2126ae7fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m478.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53430 sha256=aaa4195997b171c787dfa21e1dde97775f4c5d6ad885436042818fcf5c2b9a19\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install afinn\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "# We initialize Afinn sentiment analyzer\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfyOdCae9Gcy",
    "outputId": "fb6b7fb9-9ac0-4cf9-ea44-8a1924ad280a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I really love the new design of your website!\n",
      "Score: 3.0\n",
      "\n",
      "Text: I hate waiting in long queues.\n",
      "Score: -3.0\n",
      "\n",
      "Text: This is utterly fantastic!\n",
      "Score: 4.0\n",
      "\n",
      "Text: It's raining again. This weather is depressing.\n",
      "Score: -2.0\n",
      "\n",
      "Text: I'm not sure how I feel about the new policy.\n",
      "Score: 0.0\n",
      "\n",
      "Text: I love the absolutely wonderful performance, it was simply perfect and made me incredibly happy!\n",
      "Score: 13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of sentences to analyze\n",
    "texts = [\n",
    "    \"I really love the new design of your website!\",\n",
    "    \"I hate waiting in long queues.\",\n",
    "    \"This is utterly fantastic!\",\n",
    "    \"It's raining again. This weather is depressing.\",\n",
    "    \"I'm not sure how I feel about the new policy.\",\n",
    "    \"I love the absolutely wonderful performance, it was simply perfect and made me incredibly happy!\"\n",
    "]\n",
    "\n",
    "# Analyze the sentiment of each sentence\n",
    "for text in texts:\n",
    "    score = afinn.score(text)\n",
    "    print(f\"Text: {text}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BVxbzO8jHjo"
   },
   "source": [
    "# Sentiment Analysis with Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkFmwTel9QoZ",
    "outputId": "cd042e4e-dbf3-45ef-ec09-1e9af00273da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dudzEvRjTdY"
   },
   "source": [
    "Download the **movie_reviews** dataset from the nltk library, which is a collection of movie reviews that have been categorized as either positive or negative.\n",
    "\n",
    "It contains 2,000 movie reviews, with an equal number of positive and negative reviews. This balanced dataset is ideal for training and testing sentiment analysis algorithms, specifically the Naive Bayes Classifiers in this case to determine whether a new movie review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR59WPsp9rKw",
    "outputId": "aff298b1-8903-4fac-d40f-c021049906fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the movie review dataset from nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Run the below code to load the reviews and text preprocessing the data for modeling\n",
    "documents = [(\" \".join(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Split the dataset into the text and labels\n",
    "texts, labels = zip(*documents)\n",
    "\n",
    "# The handle_negation function is designed to preprocess text to better handle negations when performing sentiment analysis.\n",
    "# Negation words like \"not,\" \"no,\" \"never,\" and \"cannot\" can completely change the sentiment of the phrase that follows them.\n",
    "def handle_negation(text):\n",
    "    # A simple way to handle negation: attach \"not_\" to words following a negation word\n",
    "    negation_re = re.compile(r\"\\b(not|no|never|cannot)\\b[\\s]+([a-z]+)\", re.IGNORECASE)\n",
    "    return negation_re.sub(lambda match: f\"{match.group(1)}_{match.group(2)}\", text)\n",
    "\n",
    "# Apply the negation handling to your texts\n",
    "texts = [handle_negation(text) for text in texts]\n",
    "\n",
    "# This flattens a list of lists (or a mix of lists and strings) into a list of strings, needed for text processing tasks such as vectorization\n",
    "# texts = [' '.join(text) if isinstance(text, list) else text for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8ILMOSsV976Q"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize a CountVectorizer for text vectorization\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Transform the test data\n",
    "test_vectors = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "EZKwt5f8-VXf",
    "outputId": "f0e38a93-a7b4-42f4-b158-c7d46cd04cf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhN1HqxhBaid",
    "outputId": "22046caa-b529-4707-fccd-169226debd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for test data\n",
    "predictions = classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMx8Kq1XBmRc",
    "outputId": "05c5bc76-5d71-4084-d017-266fa0b92788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the model is: pos\n"
     ]
    }
   ],
   "source": [
    "# Function to predict sentiment of a new review\n",
    "def predict_sentiment(new_text):\n",
    "    new_vector = vectorizer.transform([new_text])\n",
    "    pred = classifier.predict(new_vector)\n",
    "    return pred[0]\n",
    "\n",
    "# Test the function\n",
    "sample_review = \"I absolutely loved this movie, the storyline was engaging from start to finish!\"\n",
    "print(f\"The sentiment predicted by the model is: {predict_sentiment(sample_review)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8MU7en0jjqG"
   },
   "source": [
    "# Sentiment Analysis with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATbicqQqBvH_",
    "outputId": "4856781e-517e-443c-ed59-bd4441224726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8320\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TF-IDF Vectorization instead of simple counts\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.8)\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier on the training data and labels\n",
    "logistic_classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "# Predict sentiments for test data using the trained classifier\n",
    "logistic_predictions = logistic_classifier.predict(test_vectors)\n",
    "\n",
    "# Calculate accuracy of the classifier on the test data\n",
    "logistic_accuracy = accuracy_score(test_labels, logistic_predictions)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAHemwG8F-Gg",
    "outputId": "e5a689f7-637a-4d0d-b29e-613286ca0941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the model is: pos\n"
     ]
    }
   ],
   "source": [
    "# Function to predict sentiment of a new review\n",
    "def logistic_predict_sentiment(new_text):\n",
    "    new_vector = vectorizer.transform([new_text])\n",
    "    pred = logistic_classifier.predict(new_vector)\n",
    "    return pred[0]\n",
    "\n",
    "# Test the function\n",
    "sample_review = \"The sun is shining and I'm so absolutely happy today!\"\n",
    "print(f\"The sentiment predicted by the model is: {logistic_predict_sentiment(sample_review)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_8ofVuqGCsJ",
    "outputId": "9dd3d665-612f-468b-f817-f4ccf466d5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction probability for the sample text:  [[0.48429751 0.51570249]]\n"
     ]
    }
   ],
   "source": [
    "# Check the prediction probability for a sample text\n",
    "sample_prob = logistic_classifier.predict_proba(vectorizer.transform([sample_review]))\n",
    "print(\"Prediction probability for the sample text: \", sample_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fnQI2rCju4c"
   },
   "source": [
    "# Step-by-step instructions to try out above concepts on twitter_samples dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_uVP0WnGGkN",
    "outputId": "4cd8025f-bc0f-45d7-9682-d9e1ce5f6e8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the model is: positive\n",
      "The sentiment predicted by the model is: positive\n",
      "The sentiment predicted by the model is: negative\n",
      "The sentiment predicted by the model is: negative\n",
      "The sentiment predicted by the model is: negative\n"
     ]
    }
   ],
   "source": [
    "# Download the twitter_samples dataset\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# Import twitter_samples dataset\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# Load positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Creating labelled data\n",
    "documents = []\n",
    "\n",
    "# Adding positive tweets\n",
    "for tweet in positive_tweets:\n",
    "    documents.append((tweet, \"positive\"))\n",
    "\n",
    "# Adding negative tweets\n",
    "for tweet in negative_tweets:\n",
    "    documents.append((tweet, \"negative\"))\n",
    "\n",
    "# Split the dataset into the text and labels\n",
    "texts, labels = zip(*documents)\n",
    "\n",
    "# Split data into training and test sets\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# Begin text vectorization\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.8)\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Transform the test data\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "logistic_classifier.fit(train_vectors, train_labels)\n",
    "\n",
    "\n",
    "# Predict sentiments for test data using the trained classifier\n",
    "logistic_predictions = logistic_classifier.predict(test_vectors)\n",
    "\n",
    "# Test your results with the sample tweets below\n",
    "sample_tweets = [\n",
    "    \"Absolutely loving the new update! Everything runs so smoothly and efficiently now. Great job! 👍\",\n",
    "    \"Had an amazing time at the beach today with friends. The weather was perfect! ☀️ #blessed\",\n",
    "    \"Extremely disappointed with the service at the restaurant tonight. Waited over an hour and still got the order wrong. 😡\",\n",
    "    \"Feeling really let down by the season finale. It was so rushed and left too many unanswered questions. 😞 #TVShow\",\n",
    "    \"My phone keeps crashing after the latest update. So frustrating dealing with these glitches! 😠\",\n",
    "]\n",
    "\n",
    "# Test the function\n",
    "for sentence in sample_tweets:\n",
    "    # Transform the sample tweet using the vectorizer\n",
    "    sample_vector = vectorizer.transform([sentence])\n",
    "    # Predict the sentiment of the sample tweet\n",
    "    sentiment = logistic_classifier.predict(sample_vector)\n",
    "    print(f\"The sentiment predicted by the model is: {sentiment[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxLmnyRBGVb2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
